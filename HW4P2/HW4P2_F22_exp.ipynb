{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8a_cZqr-UpV"
   },
   "source": [
    "# HW4P2: Attention-based Speech Recognition\n",
    "\n",
    "Welcome to the final assignment in 11785. In this HW, you will work on building a speech recognition system with attention. <br> <br>\n",
    "\n",
    "HW Writeup: https://piazza.com/class_profile/get_resource/l37uyxe87cq5xn/lam1lcjjj0314e <br>\n",
    "Kaggle competition link: https://www.kaggle.com/competitions/11-785-f22-hw4p2/ <br>\n",
    "LAS Paper: https://arxiv.org/pdf/1508.01211.pdf <br>\n",
    "Attention is all you need:https://arxiv.org/pdf/1706.03762.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vlev_Tvq_bRz"
   },
   "source": [
    "# Initial Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0pueIzbxUwyY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan  5 16:09:45 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA TITAN RTX    On   | 00000000:21:00.0 Off |                  N/A |\n",
      "| 62%   80C    P2   268W / 280W |   9449MiB / 24576MiB |     91%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA TITAN RTX    On   | 00000000:4B:00.0 Off |                  N/A |\n",
      "| 41%   36C    P8    30W / 280W |      8MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      3658      G   /usr/lib/xorg/Xorg                  9MiB |\n",
      "|    0   N/A  N/A      3813      G   /usr/bin/gnome-shell                4MiB |\n",
      "|    0   N/A  N/A   1294507      C   ...3/envs/realNVP/bin/python     1217MiB |\n",
      "|    0   N/A  N/A   2911249      C   ...3/envs/realNVP/bin/python      977MiB |\n",
      "|    0   N/A  N/A   3340034      C   python                           1765MiB |\n",
      "|    0   N/A  N/A   3351978      C   ...jiin/anaconda3/bin/python     3927MiB |\n",
      "|    0   N/A  N/A   3352742      C   ...da3/envs/torch/bin/python     1545MiB |\n",
      "|    1   N/A  N/A      3658      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tepaid_TDwWt"
   },
   "outputs": [],
   "source": [
    "# Install some required libraries\n",
    "# Feel free to add more if you want\n",
    "# !pip install -q python-levenshtein torchsummaryX wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lr4xGzRU-KZz"
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ZectxKF3XEVV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import Levenshtein\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "from torchsummaryX import summary\n",
    "import wandb\n",
    "from glob import glob\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OALQCI0EDCwh"
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "AqDuibMCP345"
   },
   "outputs": [],
   "source": [
    "# Global config dict. Feel free to add or change if you want.\n",
    "config = {\n",
    "    'exp_name': 'early_submission',\n",
    "    'batch_size': 96,\n",
    "    'epochs': 30,\n",
    "    'lr': 1e-3,\n",
    "    'weight_decay':  5e-6,\n",
    "    'lr_scheduler': 'stepLR',\n",
    "    'step_size': 5,\n",
    "    'locked_dropout' : 0.15\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7J4sY1OW9Pr"
   },
   "source": [
    "# Toy Data Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTsQB-pvRLLs"
   },
   "source": [
    "The toy dataset is very essential for you in this HW. The model which you will be building is complicated and you first need to make sure that it runs on the toy dataset. <br>\n",
    "In other words, you need convergence - the attention diagonal. Take a look at the write-up for this. <br>\n",
    "We have given you the following code to download the toy data and load it. You can use it the way it is. But be careful, the transcripts are different from the original data from kaggle. The toy dataset has phonemes but the actual data has characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Des4AMIaW4E8"
   },
   "outputs": [],
   "source": [
    "# !wget -q https://cmu.box.com/shared/static/wok08c2z2dp4clufhy79c5ee6jx3pyj9 --content-disposition --show-progress\n",
    "# !wget -q https://cmu.box.com/shared/static/zctr6mvh7npfn01forli8n45duhp2g85 --content-disposition --show-progress\n",
    "# !wget -q https://cmu.box.com/shared/static/m2oaek69145ljeu6srtbbb7k0ip6yfup --content-disposition --show-progress\n",
    "# !wget -q https://cmu.box.com/shared/static/owrjy0tqra3v7zq2ru7mocy2djskydy9 --content-disposition --show-progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2fLveDeiXCsb"
   },
   "outputs": [],
   "source": [
    "# # Load the toy dataset\n",
    "# X_train = np.load(\"f0176_mfccs_train.npy\")\n",
    "# X_valid = np.load(\"f0176_mfccs_dev.npy\")\n",
    "# Y_train = np.load(\"f0176_hw3p2_train.npy\")\n",
    "# Y_valid = np.load(\"f0176_hw3p2_dev.npy\")\n",
    "\n",
    "# # This is how you actually need to find out the different trancripts in a dataset. \n",
    "# # Can you think whats going on here? Why are we using a np.unique?\n",
    "# VOCAB_MAP           = dict(zip(np.unique(Y_valid), range(len(np.unique(Y_valid))))) \n",
    "# VOCAB_MAP[\"[PAD]\"]  = len(VOCAB_MAP)\n",
    "# VOCAB               = list(VOCAB_MAP.keys())\n",
    "\n",
    "# SOS_TOKEN = VOCAB_MAP[\"[SOS]\"]\n",
    "# EOS_TOKEN = VOCAB_MAP[\"[EOS]\"]\n",
    "# PAD_TOKEN = VOCAB_MAP[\"[PAD]\"]\n",
    "\n",
    "# Y_train = [np.array([VOCAB_MAP[p] for p in seq]) for seq in Y_train]\n",
    "# Y_valid = [np.array([VOCAB_MAP[p] for p in seq]) for seq in Y_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "iNjjsFqpbocR"
   },
   "outputs": [],
   "source": [
    "# # Dataset class for the Toy dataset\n",
    "# class ToyDataset(torch.utils.data.Dataset):\n",
    "\n",
    "#     def __init__(self, partition):\n",
    "\n",
    "#         if partition == \"train\":\n",
    "#             self.mfccs = X_train[:, :, :15]\n",
    "#             self.transcripts = Y_train\n",
    "\n",
    "#         elif partition == \"valid\":\n",
    "#             self.mfccs = X_valid[:, :, :15]\n",
    "#             self.transcripts = Y_valid\n",
    "\n",
    "#         assert len(self.mfccs) == len(self.transcripts)\n",
    "\n",
    "#         self.length = len(self.mfccs)\n",
    "\n",
    "#     def __len__(self):\n",
    "\n",
    "#         return self.length\n",
    "\n",
    "#     def __getitem__(self, i):\n",
    "\n",
    "#         x = torch.tensor(self.mfccs[i])\n",
    "#         y = torch.tensor(self.transcripts[i])\n",
    "\n",
    "#         return x, y\n",
    "\n",
    "#     def collate_fn(self, batch):\n",
    "\n",
    "#         x_batch, y_batch = list(zip(*batch))\n",
    "\n",
    "#         x_lens      = [x.shape[0] for x in x_batch] \n",
    "#         y_lens      = [y.shape[0] for y in y_batch] \n",
    "\n",
    "#         x_batch_pad = torch.nn.utils.rnn.pad_sequence(x_batch, batch_first=True, padding_value= EOS_TOKEN)\n",
    "#         y_batch_pad = torch.nn.utils.rnn.pad_sequence(y_batch, batch_first=True, padding_value= EOS_TOKEN) \n",
    "        \n",
    "#         return x_batch_pad, y_batch_pad, torch.tensor(x_lens), torch.tensor(y_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = ToyDataset(partition='train') #TODO\n",
    "# val_data = ToyDataset(partition='valid') # TODO : You can either use the same class with some modifications or make a new one :)\n",
    "\n",
    "# toy_train_loader =  DataLoader(train_data, shuffle=True, batch_size=config['batch_size'], collate_fn=train_data.collate_fn)\n",
    "# toy_val_loader =  DataLoader(val_data, shuffle=True, batch_size=config['batch_size'], collate_fn=train_data.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data in toy_val_loader:\n",
    "#     x, y, lx, ly = data\n",
    "#     print(x.shape, y.shape, lx.shape, ly.shape)\n",
    "#     break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWRjucnUdbQ1"
   },
   "source": [
    "# Kaggle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "fStTuAQ6XAuD"
   },
   "outputs": [],
   "source": [
    "# # TODO: Use the same Kaggle code from HW1P2, HW2P2, HW3P2\n",
    "# !pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
    "# !mkdir /root/.kaggle/\n",
    "\n",
    "# with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
    "#     f.write('') # Put your kaggle username & key here\n",
    "\n",
    "# !chmod 600 /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "nR74ooCSa664"
   },
   "outputs": [],
   "source": [
    "# Download the data\n",
    "# !kaggle competitions download -c 11-785-f22-hw4p2\n",
    "# !mkdir 'data'\n",
    "\n",
    "# !unzip -qo '11-785-f22-hw4p2.zip' -d 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5ioyn6ldQB9"
   },
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "YHix8UvpBWh7"
   },
   "outputs": [],
   "source": [
    "# These are the various characters in the transcripts of the datasetW\n",
    "VOCAB = ['<sos>',   \n",
    "         'A',   'B',    'C',    'D',    \n",
    "         'E',   'F',    'G',    'H',    \n",
    "         'I',   'J',    'K',    'L',       \n",
    "         'M',   'N',    'O',    'P',    \n",
    "         'Q',   'R',    'S',    'T', \n",
    "         'U',   'V',    'W',    'X', \n",
    "         'Y',   'Z',    \"'\",    ' ', \n",
    "         '<eos>']\n",
    "\n",
    "VOCAB_MAP = {VOCAB[i]:i for i in range(0, len(VOCAB))}\n",
    "\n",
    "SOS_TOKEN = VOCAB_MAP[\"<sos>\"]\n",
    "EOS_TOKEN = VOCAB_MAP[\"<eos>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQenneVsDLnX"
   },
   "source": [
    "# Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import AudioDataset, AudioDatasetTest\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "oi2FS_hkDQZB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  96\n",
      "Train dataset samples = 28539, batches = 298\n",
      "Val dataset samples = 2703, batches = 29\n",
      "Test dataset samples = 2620, batches = 28\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create the datasets and dataloaders\n",
    "# All these things are similar to HW3P2\n",
    "# You can reuse the same code\n",
    "\n",
    "# Create objects for the dataset class\n",
    "train_data = AudioDataset(VOCAB, VOCAB_MAP, partition='train') #TODO\n",
    "val_data = AudioDataset(VOCAB, VOCAB_MAP, partition='val') # TODO : You can either use the same class with some modifications or make a new one :)\n",
    "test_data = AudioDatasetTest() #TODO\n",
    "\n",
    "# Do NOT forget to pass in the collate function as parameter while creating the dataloader\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=config['batch_size'], collate_fn=train_data.collate_fn)#TODO\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=config['batch_size'], collate_fn=val_data.collate_fn) #TODO\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=config['batch_size'], collate_fn=test_data.collate_fn) #TODO\n",
    "\n",
    "print(\"Batch size: \", config['batch_size'])\n",
    "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
    "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
    "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))\n",
    "\n",
    "# The sanity check for shapes also are similar\n",
    "# Please remember that the only change in the dataset for this HW is the transcripts\n",
    "# So you are expected to get similar shapes like HW3P2 (Pad, pack and Oh my!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([96, 1738, 15]) torch.Size([96, 322]) torch.Size([96]) torch.Size([96])\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "for data in train_loader:\n",
    "    x, y, lx, ly = data\n",
    "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
    "    break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I--VjKlEhwi8"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uql9E6cqROvJ"
   },
   "source": [
    "In this section you will be building the LAS model from scratch. Before starting to code, please read the writeup, paper and understand the following parts completely.<br>\n",
    "- Pyramidal Bi-LSTM \n",
    "- Listener\n",
    "- Attention\n",
    "- Speller\n",
    "\n",
    "After getting a good grasp of the workings of these modules, start coding. Follow the TODOs carefully. We will also be adding some extra features to the attention mechanism like keys and values which are not originally present in LAS. So we will be creating a hybrid network based on LAS and Attention is All You Need.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCbwz0LZMWwe"
   },
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LockedDropout(nn.Module):\n",
    "    \"\"\" LockedDropout applies the same dropout mask to every time step.\n",
    "\n",
    "    Args:\n",
    "        p (float): Probability of an element in the dropout mask to be zeroed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (:class:`torch.FloatTensor` [sequence length, batch size, rnn hidden size]): Input to\n",
    "                apply dropout too.\n",
    "        \"\"\"\n",
    "        if not self.training or not self.p:\n",
    "            return x\n",
    "        x = x.clone()\n",
    "        mask = x.new_empty(1, x.size(1), x.size(2), requires_grad=False).bernoulli_(1 - self.p)\n",
    "        mask = mask.div_(1 - self.p)\n",
    "        mask = mask.expand_as(x)\n",
    "        return x * mask\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' \\\n",
    "            + 'p=' + str(self.p) + ')'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoI0zEoIMX5I"
   },
   "source": [
    "### Pyramidal Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "-F9zAQR95P55"
   },
   "outputs": [],
   "source": [
    "class pBLSTM(torch.nn.Module):\n",
    "\n",
    "    '''\n",
    "    Pyramidal BiLSTM\n",
    "    Read the write up/paper and understand the concepts and then write your implementation here.\n",
    "\n",
    "    At each step,\n",
    "    1. Pad your input if it is packed (Unpack it)\n",
    "    2. Reduce the input length dimension by concatenating feature dimension\n",
    "        (Tip: Write down the shapes and understand)\n",
    "        (i) How should  you deal with odd/even length input? \n",
    "        (ii) How should you deal with input length array (x_lens) after truncating the input?\n",
    "    3. Pack your input\n",
    "    4. Pass it into LSTM layer\n",
    "\n",
    "    To make our implementation modular, we pass 1 layer at a time.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(pBLSTM, self).__init__()\n",
    "        \n",
    "        # TODO: Initialize a single layer bidirectional LSTM with the given input_size and hidden_size\n",
    "        self.blstm = torch.nn.LSTM(input_size=4*input_size, hidden_size=hidden_size, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.downsample_factor = 2\n",
    "\n",
    "    def forward(self, x_packed): # x_packed is a PackedSequence\n",
    "\n",
    "        # TODO: Pad Packed Sequence\n",
    "        \n",
    "        # Call self.trunc_reshape() which downsamples the time steps of x and increases the feature dimensions as mentioned above\n",
    "        # self.trunc_reshape will return 2 outputs. What are they? Think about what quantites are changing.\n",
    "        # TODO: Pack Padded Sequence. What output(s) would you get?\n",
    "        # TODO: Pass the sequence through bLSTM\n",
    "\n",
    "        # What do you return?\n",
    "        x_unpacked, len_unpacked = pad_packed_sequence(x_packed, batch_first=True)\n",
    "        x_downsampled, len_downsampled = self.trunc_reshape(x_unpacked, len_unpacked)\n",
    "        x_packed = pack_padded_sequence(x_downsampled, len_downsampled, batch_first=True, enforce_sorted=False)\n",
    "        output, (hn, cn) = self.blstm(x_packed)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def trunc_reshape(self, x, x_lens): \n",
    "        # TODO: If you have odd number of timesteps, how can you handle it? (Hint: You can exclude them)\n",
    "        # TODO: Reshape x. When reshaping x, you have to reduce number of timesteps by a downsampling factor while increasing number of features by the same factor\n",
    "        # TODO: Reduce lengths by the same downsampling factor\n",
    "        max_len = x.shape[1]\n",
    "        if max_len % 2 == 1:\n",
    "            x = x[:, :-1]\n",
    "            max_len -= 1\n",
    "            \n",
    "        k = self.downsample_factor\n",
    "        x = [x[:, i:i+k].flatten(start_dim=1) for i in range(0, max_len, 2)]\n",
    "        x = torch.stack(x, dim=1)\n",
    "        # x_lens = x_lens // k\n",
    "        x_lens = torch.div(x_lens, k, rounding_mode='trunc')\n",
    "        return x, x_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rchbyjlMeB2"
   },
   "source": [
    "### Listener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "912b3sVoHr1e"
   },
   "outputs": [],
   "source": [
    "class Listener(torch.nn.Module):\n",
    "    '''\n",
    "    The Encoder takes utterances as inputs and returns latent feature representations\n",
    "    '''\n",
    "    def __init__(self, input_size, encoder_hidden_size):\n",
    "        super(Listener, self).__init__()\n",
    "\n",
    "        # The first LSTM at the very bottom\n",
    "        self.base_lstm1 = torch.nn.LSTM(input_size, encoder_hidden_size, num_layers=1, bidirectional=True, batch_first=True, dropout=0.15)#TODO: Fill this up\n",
    "        self.base_lstm2 = torch.nn.LSTM(encoder_hidden_size*2, encoder_hidden_size, num_layers=1, bidirectional=True, batch_first=True)#TODO: Fill this up\n",
    "\n",
    "\n",
    "        self.pBLSTMs = torch.nn.Sequential( # How many pBLSTMs are required?\n",
    "            # TODO: Fill this up with pBLSTMs - What should the input_size be? \n",
    "            # Hint: You are downsampling timesteps by a factor of 2, upsampling features by a factor of 2 and the LSTM is bidirectional)\n",
    "            # Optional: Dropout/Locked Dropout after each pBLSTM (Not needed for early submission)\n",
    "            # ...\n",
    "            # ...\n",
    "            pBLSTM(encoder_hidden_size, 2*encoder_hidden_size),\n",
    "            pBLSTM(2*encoder_hidden_size, 4*encoder_hidden_size),\n",
    "            pBLSTM(4*encoder_hidden_size, 8*encoder_hidden_size)\n",
    "        )\n",
    "\n",
    "        self.locked_dropout = LockedDropout(p=config['locked_dropout'])\n",
    "         \n",
    "    def forward(self, x, x_lens):\n",
    "        # Where are x and x_lens coming from? The dataloader\n",
    "        \n",
    "        # TODO: Pack Padded Sequence\n",
    "        # TODO: Pass it through the first LSTM layer (no truncation)\n",
    "        # TODO: Pad Packed Sequence\n",
    "        # TODO: Pass Sequence through the pyramidal Bi-LSTM layer\n",
    "\n",
    "        # Remember the number of output(s) each function returns\n",
    "        x_packed = pack_padded_sequence(x, x_lens, batch_first=True, enforce_sorted=False)\n",
    "        out, (hn, cn) = self.base_lstm1(x_packed)\n",
    "        x, x_lens = pad_packed_sequence(out, batch_first=True)\n",
    "        x = self.locked_dropout(x)\n",
    "\n",
    "        x_packed = pack_padded_sequence(x, x_lens, batch_first=True, enforce_sorted=False)\n",
    "        out, (hn, cn) = self.base_lstm2(x_packed)\n",
    "\n",
    "        output = self.pBLSTMs(out)\n",
    "        encoder_outputs, encoder_lens = pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        return encoder_outputs, encoder_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "0EgRqsKDI7dD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiin/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.15 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listener(\n",
      "  (base_lstm1): LSTM(15, 64, batch_first=True, dropout=0.15, bidirectional=True)\n",
      "  (base_lstm2): LSTM(128, 64, batch_first=True, bidirectional=True)\n",
      "  (pBLSTMs): Sequential(\n",
      "    (0): pBLSTM(\n",
      "      (blstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
      "    )\n",
      "    (1): pBLSTM(\n",
      "      (blstm): LSTM(512, 256, batch_first=True, bidirectional=True)\n",
      "    )\n",
      "    (2): pBLSTM(\n",
      "      (blstm): LSTM(1024, 512, batch_first=True, bidirectional=True)\n",
      "    )\n",
      "  )\n",
      "  (locked_dropout): LockedDropout(p=0.15)\n",
      ")\n",
      "torch.Size([96, 1670, 15]) torch.Size([96, 291]) torch.Size([96]) torch.Size([96])\n",
      "=========================================================================\n",
      "                       Kernel Shape    Output Shape     Params  Mult-Adds\n",
      "Layer                                                                    \n",
      "0_base_lstm1                      -    [17964, 128]    41.472k    40.448k\n",
      "1_locked_dropout                  -  [96, 291, 128]          -          -\n",
      "2_base_lstm2                      -    [17964, 128]    99.328k    98.304k\n",
      "3_pBLSTMs.0.LSTM_blstm            -     [8954, 256]   395.264k   393.216k\n",
      "4_pBLSTMs.1.LSTM_blstm            -     [4449, 512]   1.57696M  1.572864M\n",
      "5_pBLSTMs.2.LSTM_blstm            -    [2202, 1024]  6.299648M  6.291456M\n",
      "-------------------------------------------------------------------------\n",
      "                         Totals\n",
      "Total params          8.412672M\n",
      "Trainable params      8.412672M\n",
      "Non-trainable params        0.0\n",
      "Mult-Adds             8.396288M\n",
      "=========================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiin/anaconda3/envs/torch/lib/python3.9/site-packages/torchsummaryX/torchsummaryX.py:101: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sum = df.sum()\n"
     ]
    }
   ],
   "source": [
    "encoder = Listener(input_size=15, encoder_hidden_size=64).to(DEVICE) # TODO: Initialize Listener\n",
    "print(encoder)\n",
    "example_batch = next(iter(train_loader))\n",
    "print(example_batch[0].shape, example_batch[1].shape, example_batch[2].shape, example_batch[3].shape)\n",
    "summary(encoder, example_batch[0].to(DEVICE), example_batch[3])\n",
    "del encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJCpBcEmMVcZ"
   },
   "source": [
    "## Attention (Attend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6k9R7jKMRcZ"
   },
   "source": [
    "### Different ways to compute Attention\n",
    "\n",
    "1. Dot-product attention\n",
    "    * raw_weights = bmm(key, query) \n",
    "    * Optional: Scaled dot-product by normalizing with sqrt key dimension \n",
    "    * Check \"Attention is All You Need\" Section 3.2.1\n",
    "    * 1st way is what most TAs are comfortable with, but if you want to explore, check out other methods below\n",
    "\n",
    "\n",
    "2. Cosine attention\n",
    "    * raw_weights = cosine(query, key) # almost the same as dot-product xD \n",
    "\n",
    "3. Bi-linear attention\n",
    "    * W = Linear transformation (learnable parameter): d_k -> d_q\n",
    "    * raw_weights = bmm(key @ W, query)\n",
    "\n",
    "4. Multi-layer perceptron\n",
    "    * Check \"Neural Machine Translation and Sequence-to-sequence Models: A Tutorial\" Section 8.4\n",
    "\n",
    "5. Multi-Head Attention\n",
    "    * Check \"Attention is All You Need\" Section 3.2.2\n",
    "    * h = Number of heads\n",
    "    * W_Q, W_K, W_V: Weight matrix for Q, K, V (h of them in total)\n",
    "    * W_O: d_v -> d_v\n",
    "    * Reshape K: (B, T, d_k) to (B, T, h, d_k // h) and transpose to (B, h, T, d_k // h)\n",
    "    * Reshape V: (B, T, d_v) to (B, T, h, d_v // h) and transpose to (B, h, T, d_v // h)\n",
    "    * Reshape Q: (B, d_q) to (B, h, d_q // h) `\n",
    "    * raw_weights = Q @ K^T\n",
    "    * masked_raw_weights = mask(raw_weights)\n",
    "    * attention = softmax(masked_raw_weights)\n",
    "    * multi_head = attention @ V\n",
    "    * multi_head = multi_head reshaped to (B, d_v)\n",
    "    * context = multi_head @ W_O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "pqu-MUM8TjUO"
   },
   "outputs": [],
   "source": [
    "def plot_attention(attention): \n",
    "    # Function for plotting attention\n",
    "    # You need to get a diagonal plot\n",
    "    plt.clf()\n",
    "    sns.heatmap(attention, cmap='GnBu')\n",
    "    plt.show()\n",
    "\n",
    "class Attention(torch.nn.Module):\n",
    "    '''\n",
    "    Attention is calculated using the key, value (from encoder hidden states) and query from decoder.\n",
    "    Here are different ways to compute attention and context:\n",
    "\n",
    "    After obtaining the raw weights, compute and return attention weights and context as follows.:\n",
    "\n",
    "    masked_raw_weights  = mask(raw_weights) # mask out padded elements with big negative number (e.g. -1e9 or -inf in FP16)\n",
    "    attention           = softmax(masked_raw_weights)\n",
    "    context             = bmm(attention, value)\n",
    "    \n",
    "    At the end, you can pass context through a linear layer too.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, encoder_hidden_size, decoder_output_size, projection_size):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.key_projection     = nn.Linear(encoder_hidden_size*4, projection_size) # TODO: Define an nn.Linear layer which projects the encoder_hidden_state to keys\n",
    "        self.value_projection   = nn.Linear(encoder_hidden_size*4, projection_size) # TODO: Define an nn.Linear layer which projects the encoder_hidden_state to value\n",
    "        self.query_projection   = nn.Linear(decoder_output_size, projection_size) # TODO: Define an nn.Linear layer which projects the decoder_output_state to query\n",
    "        # Optional : Define an nn.Linear layer which projects the context vector\n",
    "\n",
    "        self.softmax            = nn.Softmax(dim=1) # TODO: Define a softmax layer. Think about the dimension which you need to apply \n",
    "        # Tip: What is the shape of energy? And what are those?\n",
    "        ## softmax dimension check!!!!!!!!!!!!!\n",
    "\n",
    "    # As you know, in the attention mechanism, the key, value and mask are calculated only once.\n",
    "    # This function is used to calculate them and set them to self\n",
    "    def set_key_value_mask(self, encoder_outputs, encoder_lens):\n",
    "    \n",
    "        _, encoder_max_seq_len, _ = encoder_outputs.shape\n",
    "\n",
    "        self.key      = self.key_projection(encoder_outputs) # TODO: Project encoder_outputs using key_projection to get keys\n",
    "        self.value    = self.value_projection(encoder_outputs) # TODO: Project encoder_outputs using value_projection to get values\n",
    "\n",
    "        # encoder_max_seq_len is of shape (batch_size, ) which consists of the lengths encoder output sequences in that batch\n",
    "        # The raw_weights are of shape (batch_size, timesteps)\n",
    "\n",
    "        # TODO: To remove the influence of padding in the raw_weights, we want to create a boolean mask of shape (batch_size, timesteps) \n",
    "        # The mask is False for all indicies before padding begins, True for all indices after.\n",
    "        self.padding_mask     = torch.arange(encoder_max_seq_len).unsqueeze(0) >= encoder_lens.unsqueeze(1) # TODO: You want to use a comparison between encoder_max_seq_len and encoder_lens to create this mask. \n",
    "        self.padding_mask = self.padding_mask.to(DEVICE)\n",
    "        # (Hint: Broadcasting gives you a one liner)\n",
    "        \n",
    "    def forward(self, decoder_output_embedding):\n",
    "        # key   : (batch_size, timesteps, projection_size)\n",
    "        # value : (batch_size, timesteps, projection_size)\n",
    "        # query : (batch_size, projection_size)\n",
    "\n",
    "        self.query         = self.query_projection(decoder_output_embedding) # TODO: Project the query using query_projection\n",
    "\n",
    "        # Hint: Take a look at torch.bmm for the products below \n",
    "\n",
    "        query_len = self.query.shape[1] ## projection_size\n",
    "        raw_weights        = (1/np.sqrt(query_len)) * torch.bmm(self.key, self.query.unsqueeze(-1)).squeeze(-1) # TODO: Calculate raw_weights which is the product of query and key, and is of shape (batch_size, timesteps)\n",
    "        masked_raw_weights = raw_weights.masked_fill_(self.padding_mask.bool(), -float('inf')) # TODO: Mask the raw_weights with self.padding_mask. \n",
    "        # Take a look at pytorch's masked_fill_ function (You want the fill value to be a big negative number for the softmax to make it close to 0)\n",
    "\n",
    "        attention_weights  = self.softmax(masked_raw_weights) # TODO: Calculate the attention weights, which is the softmax of raw_weights\n",
    "        # attention_weights : (batch_size, timesteps)\n",
    "        context            = torch.bmm(attention_weights.unsqueeze(1), self.value).squeeze(1) # TODO: Calculate the context - it is a product between attention_weights and value\n",
    "\n",
    "        # Hint: You might need to use squeeze/unsqueeze to make sure that your operations work with bmm\n",
    "\n",
    "        return context, attention_weights # Return the context, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78XOdWExMSi-"
   },
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TuUQTy2NMlbT"
   },
   "source": [
    "### Speller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "y7-R6BTuT8dm"
   },
   "outputs": [],
   "source": [
    "class Speller(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, decoder_hidden_size, decoder_output_size, vocab_size, attention_module= None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size         = vocab_size\n",
    "\n",
    "        self.embedding          = nn.Embedding(self.vocab_size, embed_size, padding_idx=EOS_TOKEN) # TODO: Initialize the Embedding Layer (Use the nn.Embedding Layer from torch), make sure you set the correct padding_idx  \n",
    "\n",
    "        self.lstm_cells         = torch.nn.Sequential(\n",
    "                                # Create Two LSTM Cells as per LAS Architecture\n",
    "                                # What should the input_size of the first LSTM Cell? \n",
    "                                # Hint: It takes in a combination of the character embedding and context from attention\n",
    "                                nn.LSTMCell(embed_size + attention_module.value_projection.weight.shape[0], decoder_hidden_size),\n",
    "                                nn.LSTMCell(decoder_hidden_size, decoder_output_size)\n",
    "                                )\n",
    "    \n",
    "                                # We are using LSTMCells because process individual time steps inputs and not the whole sequence.\n",
    "                                # Think why we need this in terms of the query\n",
    "        self.decoder_output_size = decoder_output_size\n",
    "\n",
    "        self.char_prob          = nn.Linear(decoder_output_size, vocab_size) # TODO: Initialize the classification layer to generate your probability distribution over all characters\n",
    "\n",
    "        self.char_prob.weight   = self.embedding.weight # Weight tying\n",
    "\n",
    "        self.attention          = attention_module\n",
    "\n",
    "    \n",
    "    def forward(self, encoder_outputs, encoder_lens, y = None, tf_rate = 1): \n",
    "\n",
    "        '''\n",
    "        Args: \n",
    "            embedding: Attention embeddings \n",
    "            hidden_list: List of Hidden States for the LSTM Cells\n",
    "        ''' \n",
    "\n",
    "        batch_size, encoder_max_seq_len, _ = encoder_outputs.shape\n",
    "\n",
    "        if self.training:\n",
    "            timesteps     = y.shape[1] # The number of timesteps is the sequence of length of your transcript during training\n",
    "            label_embed   = self.embedding(y) # Embeddings of the transcript, when we want to use teacher forcing\n",
    "        else:\n",
    "            timesteps     = 600 # 600 is a design choice that we recommend, however you are free to experiment.\n",
    "        \n",
    "        # INITS\n",
    "        predictions     = []\n",
    "\n",
    "        # Initialize the first character input to your decoder, SOS\n",
    "        char            = torch.full((batch_size,), fill_value=SOS_TOKEN, dtype= torch.long).to(DEVICE) \n",
    "\n",
    "        # Initialize a list to keep track of LSTM Cell Hidden and Cell Memory States, to None\n",
    "        hidden_states   = [None]*len(self.lstm_cells) \n",
    "\n",
    "        attention_plot          = []\n",
    "        context                 = torch.zeros(batch_size, self.decoder_output_size).to(DEVICE) # TODO: Initialize context (You have a few choices, refer to the writeup )\n",
    "        attention_weights       = torch.zeros(batch_size, encoder_max_seq_len) # Attention Weights are zero if not using Attend Module\n",
    "\n",
    "        # Set Attention Key, Value, Padding Mask just once\n",
    "        if self.attention != None:\n",
    "            self.attention.set_key_value_mask(encoder_outputs, encoder_lens)\n",
    "\n",
    "\n",
    "        for t in range(timesteps):\n",
    "            char_embed = self.embedding(char) #TODO: Generate the embedding for the character at timestep t\n",
    "\n",
    "            if self.training and t > 0:\n",
    "                # TODO: We want to decide which embedding to use as input for the decoder during training\n",
    "                # We can use the embedding of the transcript character or the embedding of decoded/predicted character, from the previous timestep \n",
    "                # Using the embedding of the transcript character is teacher forcing, it is very important for faster convergence\n",
    "                # Use a comparison between a random probability and your teacher forcing rate, to decide which embedding to use\n",
    "\n",
    "                char_embed = label_embed[:, t-1, :] # TODO\n",
    "      \n",
    "            decoder_input_embedding = torch.cat([char_embed, context], dim=1) # TODO: What do we want to concatenate as input to the decoder? (Use torch.cat)\n",
    "            # char_embed : (batch_size, embed_size)\n",
    "            # context : (batch_size, projection_size)\n",
    "            \n",
    "            # Loop over your lstm cells\n",
    "            # Each lstm cell takes in an embedding \n",
    "            for i in range(len(self.lstm_cells)):\n",
    "                # An LSTM Cell returns (h,c) -> h = hidden state, c = cell memory state\n",
    "                # Using 2 LSTM Cells is akin to a 2 layer LSTM looped through t timesteps \n",
    "                # The second LSTM Cell takes in the output hidden state of the first LSTM Cell (from the current timestep) as Input, along with the hidden and cell states of the cell from the previous timestep\n",
    "                hidden_states[i] = self.lstm_cells[i](decoder_input_embedding, hidden_states[i]) \n",
    "                decoder_input_embedding = hidden_states[i][0]\n",
    "\n",
    "            # The output embedding from the decoder is the hidden state of the last LSTM Cell\n",
    "            decoder_output_embedding = hidden_states[-1][0] ## s(t) : (batch_size, decoder_output_size)\n",
    "\n",
    "            # We compute attention from the output of the last LSTM Cell\n",
    "            if self.attention != None:\n",
    "                context, attention_weights = self.attention(decoder_output_embedding) # The returned query is the projected query\n",
    "\n",
    "            # context : (batch_size, projection_size)\n",
    "            # attention weights : (batch_size, timesteps)\n",
    "            attention_plot.append(attention_weights[0].detach().cpu())\n",
    "\n",
    "            output_embedding     = torch.cat([decoder_output_embedding, context], dim=1) # TODO: Concatenate the projected query with context for the output embedding\n",
    "            # Hint: How can you get the projected query from attention\n",
    "            # If you are not using attention, what will you use instead of query?\n",
    "\n",
    "            char_prob            = self.char_prob(output_embedding)\n",
    "            \n",
    "            # Append the character probability distribution to the list of predictions \n",
    "            predictions.append(char_prob)\n",
    "\n",
    "            char = torch.argmax(char_prob, dim=1) # TODO: Get the predicted character for the next timestep from the probability distribution \n",
    "            # (Hint: Use Greedy Decoding for starters)\n",
    "\n",
    "        attention_plot  = torch.stack(attention_plot) # TODO: Stack list of attetion_plots \n",
    "        predictions     = torch.stack(predictions) # TODO: Stack list of predictions \n",
    "\n",
    "        return predictions, attention_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMgncQmVMnCO"
   },
   "source": [
    "## Sequence-to-Sequence Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWWzurvXM0iv"
   },
   "source": [
    "### LAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "zcTC4cK95TYT"
   },
   "outputs": [],
   "source": [
    "class LAS(torch.nn.Module):\n",
    "    def __init__(self, input_size, encoder_hidden_size, \n",
    "                 vocab_size, embed_size,\n",
    "                 decoder_hidden_size, decoder_output_size,\n",
    "                 projection_size= 128):\n",
    "        \n",
    "        super(LAS, self).__init__()\n",
    "\n",
    "        self.encoder        = Listener(input_size=15, encoder_hidden_size=64) # TODO: Initialize Encoder\n",
    "        attention_module    = Attention(encoder_hidden_size, decoder_output_size, projection_size) # TODO: Initialize Attention\n",
    "        self.decoder        = Speller(embed_size, decoder_hidden_size, decoder_output_size, vocab_size, attention_module) # TODO: Initialize Decoder, make sure you pass the attention module \n",
    "\n",
    "    def forward(self, x, x_lens, y = None, tf_rate = 1):\n",
    "\n",
    "        encoder_outputs, encoder_lens = self.encoder(x, x_lens) # from Listener\n",
    "        predictions, attention_plot = self.decoder(encoder_outputs, encoder_lens, y, tf_rate)\n",
    "        predictions = predictions.permute(1,0,2)\n",
    "        \n",
    "        return predictions, attention_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHMzR6fLht5n"
   },
   "source": [
    "# Training Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TI2AKhQ6YP6F"
   },
   "source": [
    "## Model Setup\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "IS-YUHQlYQmL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiin/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.15 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/home/jiin/anaconda3/envs/torch/lib/python3.9/site-packages/torchsummaryX/torchsummaryX.py:101: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sum = df.sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================================================\n",
      "                                               Kernel Shape    Output Shape  \\\n",
      "Layer                                                                         \n",
      "0_encoder.LSTM_base_lstm1                                 -    [17964, 128]   \n",
      "1_encoder.LockedDropout_locked_dropout                    -  [96, 291, 128]   \n",
      "2_encoder.LSTM_base_lstm2                                 -    [17964, 128]   \n",
      "3_encoder.pBLSTMs.0.LSTM_blstm                            -     [8954, 256]   \n",
      "4_encoder.pBLSTMs.1.LSTM_blstm                            -     [4449, 512]   \n",
      "...                                                     ...             ...   \n",
      "1750_decoder.lstm_cells.LSTMCell_0                        -       [96, 512]   \n",
      "1751_decoder.lstm_cells.LSTMCell_1                        -       [96, 128]   \n",
      "1752_decoder.attention.Linear_query_projection   [128, 128]       [96, 128]   \n",
      "1753_decoder.attention.Softmax_softmax                    -        [96, 36]   \n",
      "1754_decoder.Linear_char_prob                     [256, 30]        [96, 30]   \n",
      "\n",
      "                                                  Params  Mult-Adds  \n",
      "Layer                                                                \n",
      "0_encoder.LSTM_base_lstm1                        41.472k    40.448k  \n",
      "1_encoder.LockedDropout_locked_dropout                 -          -  \n",
      "2_encoder.LSTM_base_lstm2                        99.328k    98.304k  \n",
      "3_encoder.pBLSTMs.0.LSTM_blstm                  395.264k   393.216k  \n",
      "4_encoder.pBLSTMs.1.LSTM_blstm                  1.57696M  1.572864M  \n",
      "...                                                  ...        ...  \n",
      "1750_decoder.lstm_cells.LSTMCell_0                     -  1.835008M  \n",
      "1751_decoder.lstm_cells.LSTMCell_1                     -    327.68k  \n",
      "1752_decoder.attention.Linear_query_projection         -    16.384k  \n",
      "1753_decoder.attention.Softmax_softmax                 -          -  \n",
      "1754_decoder.Linear_char_prob                          -      7.68k  \n",
      "\n",
      "[1755 rows x 4 columns]\n",
      "-------------------------------------------------------------------------------------------------\n",
      "                           Totals\n",
      "Total params           10.874782M\n",
      "Trainable params       10.874782M\n",
      "Non-trainable params          0.0\n",
      "Mult-Adds             647.245824M\n",
      "=================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kernel Shape</th>\n",
       "      <th>Output Shape</th>\n",
       "      <th>Params</th>\n",
       "      <th>Mult-Adds</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0_encoder.LSTM_base_lstm1</th>\n",
       "      <td>-</td>\n",
       "      <td>[17964, 128]</td>\n",
       "      <td>41472.0</td>\n",
       "      <td>40448.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_encoder.LockedDropout_locked_dropout</th>\n",
       "      <td>-</td>\n",
       "      <td>[96, 291, 128]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_encoder.LSTM_base_lstm2</th>\n",
       "      <td>-</td>\n",
       "      <td>[17964, 128]</td>\n",
       "      <td>99328.0</td>\n",
       "      <td>98304.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_encoder.pBLSTMs.0.LSTM_blstm</th>\n",
       "      <td>-</td>\n",
       "      <td>[8954, 256]</td>\n",
       "      <td>395264.0</td>\n",
       "      <td>393216.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_encoder.pBLSTMs.1.LSTM_blstm</th>\n",
       "      <td>-</td>\n",
       "      <td>[4449, 512]</td>\n",
       "      <td>1576960.0</td>\n",
       "      <td>1572864.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1750_decoder.lstm_cells.LSTMCell_0</th>\n",
       "      <td>-</td>\n",
       "      <td>[96, 512]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1835008.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751_decoder.lstm_cells.LSTMCell_1</th>\n",
       "      <td>-</td>\n",
       "      <td>[96, 128]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>327680.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752_decoder.attention.Linear_query_projection</th>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>[96, 128]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16384.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1753_decoder.attention.Softmax_softmax</th>\n",
       "      <td>-</td>\n",
       "      <td>[96, 36]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754_decoder.Linear_char_prob</th>\n",
       "      <td>[256, 30]</td>\n",
       "      <td>[96, 30]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7680.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1755 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Kernel Shape    Output Shape  \\\n",
       "Layer                                                                         \n",
       "0_encoder.LSTM_base_lstm1                                 -    [17964, 128]   \n",
       "1_encoder.LockedDropout_locked_dropout                    -  [96, 291, 128]   \n",
       "2_encoder.LSTM_base_lstm2                                 -    [17964, 128]   \n",
       "3_encoder.pBLSTMs.0.LSTM_blstm                            -     [8954, 256]   \n",
       "4_encoder.pBLSTMs.1.LSTM_blstm                            -     [4449, 512]   \n",
       "...                                                     ...             ...   \n",
       "1750_decoder.lstm_cells.LSTMCell_0                        -       [96, 512]   \n",
       "1751_decoder.lstm_cells.LSTMCell_1                        -       [96, 128]   \n",
       "1752_decoder.attention.Linear_query_projection   [128, 128]       [96, 128]   \n",
       "1753_decoder.attention.Softmax_softmax                    -        [96, 36]   \n",
       "1754_decoder.Linear_char_prob                     [256, 30]        [96, 30]   \n",
       "\n",
       "                                                   Params  Mult-Adds  \n",
       "Layer                                                                 \n",
       "0_encoder.LSTM_base_lstm1                         41472.0    40448.0  \n",
       "1_encoder.LockedDropout_locked_dropout                NaN        NaN  \n",
       "2_encoder.LSTM_base_lstm2                         99328.0    98304.0  \n",
       "3_encoder.pBLSTMs.0.LSTM_blstm                   395264.0   393216.0  \n",
       "4_encoder.pBLSTMs.1.LSTM_blstm                  1576960.0  1572864.0  \n",
       "...                                                   ...        ...  \n",
       "1750_decoder.lstm_cells.LSTMCell_0                    NaN  1835008.0  \n",
       "1751_decoder.lstm_cells.LSTMCell_1                    NaN   327680.0  \n",
       "1752_decoder.attention.Linear_query_projection        NaN    16384.0  \n",
       "1753_decoder.attention.Softmax_softmax                NaN        NaN  \n",
       "1754_decoder.Linear_char_prob                         NaN     7680.0  \n",
       "\n",
       "[1755 rows x 4 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline LAS has the following configuration:\n",
    "# Encoder bLSTM/pbLSTM Hidden Dimension of 512 (256 per direction)\n",
    "# Decoder Embedding Layer Dimension of 256\n",
    "# Decoder Hidden Dimension of 512 \n",
    "# Decoder Output Dimension of 128\n",
    "# Attention Projection Size of 128\n",
    "# Feel Free to Experiment with this \n",
    "\n",
    "model = LAS(\n",
    "    input_size=15, encoder_hidden_size=256, \n",
    "    vocab_size=len(VOCAB), embed_size=256,\n",
    "    decoder_hidden_size=512, decoder_output_size=128,\n",
    "    projection_size= 128\n",
    "    # Initialize your model \n",
    "    # Read the paper and think about what dimensions should be used\n",
    "    # You can experiment on these as well, but they are not requried for the early submission\n",
    "    # Remember that if you are using weight tying, some sizes need to be the same\n",
    ")\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "# print(model)\n",
    "\n",
    "summary(model, \n",
    "        x= example_batch[0].to(DEVICE), \n",
    "        x_lens= example_batch[3], \n",
    "        y= example_batch[1].to(DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDmcYul-YSdC"
   },
   "source": [
    "## Optimizer, Scheduler, Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "_HwmgDSvbtmd"
   },
   "outputs": [],
   "source": [
    "optimizer   = torch.optim.Adam(model.parameters(), lr= config['lr'], amsgrad= True, weight_decay=config['weight_decay'])\n",
    "criterion   = torch.nn.CrossEntropyLoss(reduction='none') # Why are we using reduction = 'none' ? \n",
    "scaler      = torch.cuda.amp.GradScaler()\n",
    "scheduler   = torch.optim.lr_scheduler.StepLR(optimizer, step_size=config['step_size'], gamma=0.8)\n",
    "\n",
    "# Optional: Create a custom class for a Teacher Force Schedule "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baHCja89YV-m"
   },
   "source": [
    "# Levenshtein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "GDYZnnLbqJ8J"
   },
   "outputs": [],
   "source": [
    "# We have given you this utility function which takes a sequence of indices and converts them to a list of characters\n",
    "def indices_to_chars(indices, vocab):\n",
    "    tokens = []\n",
    "    for i in indices: # This loops through all the indices\n",
    "        if int(i) == SOS_TOKEN: # If SOS is encountered, dont add it to the final list\n",
    "            continue\n",
    "        elif int(i) == EOS_TOKEN: # If EOS is encountered, stop the decoding process\n",
    "            break\n",
    "        else:\n",
    "            tokens.append(vocab[i])\n",
    "    return tokens\n",
    "\n",
    "# To make your life more easier, we have given the Levenshtein distantce / Edit distance calculation code\n",
    "def calc_edit_distance(predictions, y, ly, vocab= VOCAB, print_example= False):\n",
    "\n",
    "    dist                = 0\n",
    "    batch_size, seq_len = predictions.shape\n",
    "\n",
    "    for batch_idx in range(batch_size): \n",
    "\n",
    "        y_sliced    = indices_to_chars(y[batch_idx,0:ly[batch_idx]], vocab)\n",
    "        pred_sliced = indices_to_chars(predictions[batch_idx], vocab)\n",
    "\n",
    "        # Strings - When you are using characters from the AudioDataset\n",
    "        y_string    = ''.join(y_sliced)\n",
    "        pred_string = ''.join(pred_sliced)\n",
    "        \n",
    "        dist        += Levenshtein.distance(pred_string, y_string)\n",
    "        # Comment the above abd uncomment below for toy dataset \n",
    "        # dist      += Levenshtein.distance(y_sliced, pred_sliced)\n",
    "\n",
    "    if print_example: \n",
    "        # Print y_sliced and pred_sliced if you are using the toy dataset\n",
    "        print(\"Ground Truth : \", y_string)\n",
    "        print(\"Prediction   : \", pred_string)\n",
    "        # print(\"Ground Truth : \", y_sliced)\n",
    "        # print(\"Prediction   : \", pred_sliced)\n",
    "        \n",
    "    dist/=batch_size\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zjfF88iZ4Nc"
   },
   "source": [
    "# Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "wIXzhQclhs98"
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, teacher_forcing_rate):\n",
    "\n",
    "    model.train()\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
    "\n",
    "    running_loss        = 0.0\n",
    "    running_perplexity  = 0.0\n",
    "    \n",
    "    for i, (x, y, lx, ly) in enumerate(dataloader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x, y, lx, ly = x.to(DEVICE), y.to(DEVICE), lx, ly\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "\n",
    "            predictions, attention_plot = model(x, lx, y= y, tf_rate= teacher_forcing_rate)\n",
    "\n",
    "            # Predictions are of Shape (batch_size, timesteps, vocab_size). \n",
    "            # Transcripts are of shape (batch_size, timesteps) Which means that you have batch_size amount of batches with timestep number of tokens.\n",
    "            # So in total, you have batch_size*timesteps amount of characters.\n",
    "            # Similarly, in predictions, you have batch_size*timesteps amount of probability distributions.\n",
    "            # How do you need to modify transcipts and predictions so that you can calculate the CrossEntropyLoss? Hint: Use Reshape/View and read the docs\n",
    "            loss        = criterion(predictions.permute(0,2,1), y) # TODO: Cross Entropy Loss\n",
    "\n",
    "            # print(loss.shape, ly.shape)  #(batch, max_len) (batch)\n",
    "            mask        = torch.ones(loss.shape).to(DEVICE) # TODO: Create a boolean mask using the lengths of your transcript that remove the influence of padding indices (in transcripts) in the loss \n",
    "            for b in range(loss.shape[0]):\n",
    "                mask[b][ly[b]:] = 0\n",
    "            masked_loss = (mask * loss).mean() # Product between the mask and the loss, divided by the mask's sum. Hint: You may want to reshape the mask too \n",
    "            perplexity  = torch.exp(masked_loss) # Perplexity is defined the exponential of the loss\n",
    "\n",
    "            running_loss        += masked_loss.item()\n",
    "            running_perplexity  += perplexity.item()\n",
    "        \n",
    "        # Backward on the masked loss\n",
    "        scaler.scale(masked_loss).backward()\n",
    "\n",
    "        # Optional: Use torch.nn.utils.clip_grad_norm to clip gradients to prevent them from exploding, if necessary\n",
    "        # If using with mixed precision, unscale the Optimizer First before doing gradient clipping\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "\n",
    "        batch_bar.set_postfix(\n",
    "            loss=\"{:.04f}\".format(running_loss/(i+1)),\n",
    "            perplexity=\"{:.04f}\".format(running_perplexity/(i+1)),\n",
    "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])),\n",
    "            tf_rate='{:.02f}'.format(teacher_forcing_rate))\n",
    "        batch_bar.update()\n",
    "\n",
    "        del x, y, lx, ly\n",
    "        # torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    running_loss /= len(dataloader)\n",
    "    running_perplexity /= len(dataloader)\n",
    "    batch_bar.close()\n",
    "\n",
    "    return running_loss, running_perplexity, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "jzpCjd9R5VYV"
   },
   "outputs": [],
   "source": [
    "def validate(model, dataloader):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc=\"Val\")\n",
    "\n",
    "    running_lev_dist = 0.0\n",
    "\n",
    "    for i, (x, y, lx, ly) in enumerate(dataloader):\n",
    "\n",
    "        x, y, lx, ly = x.to(DEVICE), y.to(DEVICE), lx, ly\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            predictions, attentions = model(x, lx, y = None)\n",
    "\n",
    "        # Greedy Decoding\n",
    "        greedy_predictions   = torch.argmax(predictions, dim=2) # TODO: How do you get the most likely character from each distribution in the batch?\n",
    "\n",
    "        # Calculate Levenshtein Distance\n",
    "        running_lev_dist    += calc_edit_distance(greedy_predictions, y, ly, VOCAB, print_example = False) # You can use print_example = True for one specific index i in your batches if you want\n",
    "\n",
    "        batch_bar.set_postfix(\n",
    "            dist=\"{:.04f}\".format(running_lev_dist/(i+1)))\n",
    "        batch_bar.update()\n",
    "\n",
    "        del x, y, lx, ly\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    batch_bar.close()\n",
    "    running_lev_dist /= len(dataloader)\n",
    "\n",
    "    return running_lev_dist#, running_loss, running_perplexity, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9uRTbbPkeL-"
   },
   "source": [
    "# Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "qVVo8bAab_5N"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjiin\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jiin/.netrc\n"
     ]
    }
   ],
   "source": [
    "# Login to Wandb\n",
    "# Initialize your Wandb Run Here\n",
    "# Optional: Save your model architecture in a txt file, and save the file to Wandb\n",
    "import wandb\n",
    "wandb.login(key=\"db668044fcf11bc7352d5c79ac0deb75ac86a16b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:rn8hnda3) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e0ead3907f40639d41bfef076c1ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">early_submission</strong>: <a href=\"https://wandb.ai/jiin/hw4p2-ablations/runs/rn8hnda3\" target=\"_blank\">https://wandb.ai/jiin/hw4p2-ablations/runs/rn8hnda3</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230105_154745-rn8hnda3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:rn8hnda3). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/usr/SSD/jiin/CMU/IDL/HW4P2/wandb/run-20230105_155929-20hscqb2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jiin/hw4p2-ablations/runs/20hscqb2\" target=\"_blank\">early_submission</a></strong> to <a href=\"https://wandb.ai/jiin/hw4p2-ablations\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    name = config['exp_name'], ## Wandb creates random run names if you skip this field\n",
    "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
    "    # run_id = ### Insert specific run id here if you want to resume a previous run\n",
    "    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
    "    project = \"hw4p2-ablations\", ### Project should be created in your wandb account \n",
    "    config = config ### Wandb Config for your run\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99rJQUdPkCUq"
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s12S_bPMcguA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef51b04a822465386568552af99dfdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/298 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3125294/3346472853.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Call train and validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_perplexity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mvalid_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mcurr_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3125294/1792887747.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, criterion, optimizer, teacher_forcing_rate)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Backward on the masked loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Optional: Use torch.nn.utils.clip_grad_norm to clip gradients to prevent them from exploding, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_lev_dist = float(\"inf\")\n",
    "tf_rate = 1.0\n",
    "\n",
    "for epoch in range(0, config['epochs']):\n",
    "    \n",
    "    print(\"\\nEpoch: {}/{}\".format(epoch+1, config['epochs']))\n",
    "\n",
    "    # Call train and validate \n",
    "    train_loss, train_perplexity, attention_plot = train(model, train_loader, criterion, optimizer, tf_rate)\n",
    "    valid_dist = validate(model, val_loader)\n",
    "    curr_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    # Print your metrics\n",
    "    print('Train loss = {:.4f}'.format(train_loss))\n",
    "    print('Valid distance = {:.4f}'.format(valid_dist))\n",
    "\n",
    "    # Plot Attention \n",
    "    plot_attention(attention_plot)\n",
    "\n",
    "    # Log metrics to Wandb\n",
    "    wandb.log({\"train_loss\":train_loss, \"train_perplexity\":train_perplexity, \"validation_dist\": valid_dist, \"learning_rate\": curr_lr})\n",
    "\n",
    "    # Optional: Scheduler Step / Teacher Force Schedule Step\n",
    "    scheduler.step()\n",
    "    tf_rate = tf_rate * 0.9\n",
    "\n",
    "\n",
    "    if valid_dist <= best_lev_dist:\n",
    "        best_lev_dist = valid_dist\n",
    "        # Save your model checkpoint here\n",
    "        torch.save({'state_dict':model.state_dict(),\n",
    "        'optimizer_state':optimizer.state_dict(),\n",
    "        'epoch': epoch}, 'models/{}_checkpoint.pth'.format(config['exp_name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3iickk_kJNB"
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7HihzA4ViiR"
   },
   "outputs": [],
   "source": [
    "# Optional: Load your best model Checkpoint here\n",
    "model = LAS(\n",
    "    input_size=15, encoder_hidden_size=256, \n",
    "    vocab_size=len(VOCAB), embed_size=256,\n",
    "    decoder_hidden_size=512, decoder_output_size=128,\n",
    "    projection_size= 128\n",
    ").to(DEVICE)\n",
    "\n",
    "model.load_state_dict = torch.load('models/{}_checkpoint.pth'.format(config['exp_name'])['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mTBD49c_kKs3"
   },
   "outputs": [],
   "source": [
    "# TODO: Create a testing function similar to validation \n",
    "# TODO: Create a file with all predictions \n",
    "# TODO: Submit to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    all_predictions = []\n",
    "\n",
    "\n",
    "    for i, (x, y, lx, ly) in tqdm(enumerate(dataloader)):\n",
    "\n",
    "        x, y, lx, ly = x.to(DEVICE), y.to(DEVICE), lx, ly\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            predictions, attentions = model(x, lx, y = None)\n",
    "\n",
    "        # Greedy Decoding\n",
    "        greedy_predictions   = torch.argmax(predictions, dim=2) # TODO: How do you get the most likely character from each distribution in the batch?\n",
    "\n",
    "        all_predictions.append(greedy_predictions)\n",
    "\n",
    "        del x, y, lx, ly\n",
    "\n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Vlev_Tvq_bRz",
    "lr4xGzRU-KZz",
    "OALQCI0EDCwh",
    "X7J4sY1OW9Pr",
    "YWRjucnUdbQ1",
    "i5ioyn6ldQB9",
    "gQenneVsDLnX",
    "lCbwz0LZMWwe",
    "XoI0zEoIMX5I",
    "_rchbyjlMeB2",
    "JJCpBcEmMVcZ",
    "f6k9R7jKMRcZ"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "924e00a5e389196e5ec413a0a9dc634853c07f75070e4b6a6f9e4e6ec9e3f2c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
